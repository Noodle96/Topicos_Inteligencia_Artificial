{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc27de6a-1b57-426b-a4c5-2de0ba06d6c5",
   "metadata": {},
   "source": [
    "### Evaluación de Modelos: Parte Teórica de la Métrica Precisión\n",
    "\n",
    "La **precisión** es una métrica clave para evaluar el rendimiento de un modelo de clasificación. \n",
    "Su objetivo principal es medir qué tan confiables son las predicciones positivas realizadas por el modelo. \n",
    "Es especialmente útil en escenarios donde los **falsos positivos** tienen un impacto significativo, como en detección de fraudes o clasificación de correos como spam.\n",
    "\n",
    "---\n",
    "\n",
    "#### Definición Formal\n",
    "\n",
    "La precisión se calcula como:\n",
    "\n",
    "$$\n",
    "\\text{Precisión} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Componentes del Cálculo\n",
    "\n",
    "- **True Positives (TP):** Instancias correctamente clasificadas como positivas.\n",
    "- **False Positives (FP):** Instancias clasificadas como positivas de manera incorrecta.\n",
    "\n",
    "---\n",
    "\n",
    "#### Intuición\n",
    "\n",
    "- Una alta precisión indica que cuando el modelo predice una clase positiva, lo hace con mucha certeza.\n",
    "- Una baja precisión sugiere que el modelo clasifica erróneamente muchas instancias negativas como positivas.\n",
    "\n",
    "---\n",
    "\n",
    "#### Ejemplo Práctico\n",
    "\n",
    "Imagina que un modelo clasifica correos como **Spam** (positivo) o **No Spam** (negativo). Si el modelo identifica 50 correos como Spam, pero 10 de ellos son correos normales, la precisión sería:\n",
    "\n",
    "$$\n",
    "\\text{Precisión} = \\frac{40}{40 + 10} = 0.8 \\, (80\\%)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Precisión en Clasificación Multiclase\n",
    "\n",
    "En problemas multiclase, se calcula la precisión para cada clase individualmente usando el enfoque **uno contra todos (One-vs-All)**. Luego, se puede:\n",
    "\n",
    "1. **Reportar la precisión de cada clase.**\n",
    "2. **Calcular un promedio** (ponderado o macro) para obtener una visión general del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "#### Fórmulas de Promedio:\n",
    "\n",
    "- **Promedio Macro:** Promedio simple de las precisiones de todas las clases.\n",
    "- **Promedio Ponderado:** Promedio de las precisiones, ponderado por el soporte (número de instancias) de cada clase.\n",
    "\n",
    "---\n",
    "\n",
    "#### Usos y Limitaciones\n",
    "\n",
    "- **Usos:** Escenarios donde los falsos positivos son costosos (e.g., diagnóstico médico, sistemas de recomendación).\n",
    "- **Limitación:** La precisión no considera los falsos negativos. Si el modelo omite muchas instancias positivas, podría tener una alta precisión pero un bajo desempeño general.\n",
    "\n",
    "---\n",
    "\n",
    "### Detalle de True Positives, False Positives, False Negatives y True Negatives\n",
    "\n",
    "#### Definiciones\n",
    "\n",
    "- **True Positives (TP):** Predicciones correctas de la clase positiva.\n",
    "- **False Positives (FP):** Predicciones incorrectas donde se clasificó como positivo algo que no lo es.\n",
    "- **False Negatives (FN):** Casos donde la etiqueta real es positiva, pero el modelo predijo negativo.\n",
    "- **True Negatives (TN):** Predicciones correctas de la clase negativa.\n",
    "\n",
    "---\n",
    "\n",
    "#### Ejemplo: Clasificación Binaria\n",
    "\n",
    "Supongamos un modelo que clasifica correos como **Spam** o **No Spam** con los siguientes resultados:\n",
    "\n",
    "| Índice | Etiqueta Real | Predicción |\n",
    "|--------|---------------|------------|\n",
    "| 1      | Spam          | Spam       |\n",
    "| 2      | No Spam       | Spam       |\n",
    "| 3      | Spam          | No Spam    |\n",
    "| 4      | No Spam       | No Spam    |\n",
    "| 5      | Spam          | Spam       |\n",
    "\n",
    "Cálculo de métricas:\n",
    "- TP = 2 (índices 1 y 5)\n",
    "- FP = 1 (índice 2)\n",
    "- FN = 1 (índice 3)\n",
    "- TN = 1 (índice 4)\n",
    "\n",
    "---\n",
    "\n",
    "#### Cálculo Manual en Python\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37b95500-938a-4ce5-ba95-e2017c12cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 2, FP: 1, FN: 1, TN: 1\n",
      "Precisión (manual): 0.67\n",
      "Precisión (sklearn): 0.67\n"
     ]
    }
   ],
   "source": [
    "# Datos simulados\n",
    "y_test_bin = [1, 0, 1, 0, 1]  # Etiquetas reales (1: Spam, 0: No Spam)\n",
    "y_pred_bin = [1, 1, 0, 0, 1]  # Predicciones del modelo\n",
    "\n",
    "# Cálculo manual\n",
    "TP = sum((yt == 1) and (yp == 1) for yt, yp in zip(y_test_bin, y_pred_bin))\n",
    "FP = sum((yt == 0) and (yp == 1) for yt, yp in zip(y_test_bin, y_pred_bin))\n",
    "FN = sum((yt == 1) and (yp == 0) for yt, yp in zip(y_test_bin, y_pred_bin))\n",
    "TN = sum((yt == 0) and (yp == 0) for yt, yp in zip(y_test_bin, y_pred_bin))\n",
    "\n",
    "# Cálculo manual de precisión\n",
    "precision_manual = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "# Mostrar resultados manuales\n",
    "print(f\"TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "print(f\"Precisión (manual): {precision_manual:.2f}\")\n",
    "\n",
    "# Cálculo usando sklearn\n",
    "from sklearn.metrics import precision_score\n",
    "precision_bin = precision_score(y_test_bin, y_pred_bin)\n",
    "print(f\"Precisión (sklearn): {precision_bin:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b88d3c-e9c0-436c-aed6-dec8e82dcade",
   "metadata": {},
   "source": [
    "#### Clasificación Multiclase\n",
    "\n",
    "Con las clases **Gato**, **Perro** y **Conejo**, consideramos 10 registros:\n",
    "\n",
    "| Índice | Etiqueta Real | Predicción |\n",
    "|--------|---------------|------------|\n",
    "| 1      | Gato          | Gato       |\n",
    "| 2      | Perro         | Conejo     |\n",
    "| 3      | Conejo        | Perro      |\n",
    "| 4      | Gato          | Gato       |\n",
    "| 5      | Perro         | Perro      |\n",
    "| 6      | Conejo        | Conejo     |\n",
    "| 7      | Gato          | Perro      |\n",
    "| 8      | Perro         | Gato       |\n",
    "| 9      | Conejo        | Conejo     |\n",
    "| 10     | Perro         | Perro      |\n",
    "\n",
    "---\n",
    "\n",
    "#### Cálculo Manual y con `sklearn` en Python\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6cbe64f-33d0-4034-a831-0714b2350b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase: Gato, Métricas: {'TP': 2, 'FP': 1, 'FN': 1, 'Soporte': 3, 'Precisión (manual)': 0.6666666666666666}\n",
      "Clase: Perro, Métricas: {'TP': 2, 'FP': 2, 'FN': 2, 'Soporte': 4, 'Precisión (manual)': 0.5}\n",
      "Clase: Conejo, Métricas: {'TP': 2, 'FP': 1, 'FN': 1, 'Soporte': 3, 'Precisión (manual)': 0.6666666666666666}\n",
      "\n",
      "Precisión Macro (manual): 0.61\n",
      "Precisión Ponderada (manual): 0.60\n",
      "\n",
      "Precisión Macro (sklearn): 0.61\n",
      "Precisión Ponderada (sklearn): 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Datos simulados\n",
    "y_test_multi = ['Gato', 'Perro', 'Conejo', 'Gato', 'Perro', 'Conejo', 'Gato', 'Perro', 'Conejo', 'Perro']\n",
    "y_pred_multi = ['Gato', 'Conejo', 'Perro', 'Gato', 'Perro', 'Conejo', 'Perro', 'Gato', 'Conejo', 'Perro']\n",
    "\n",
    "# Cálculo manual por clase\n",
    "clases = ['Gato', 'Perro', 'Conejo']\n",
    "resultados = {}\n",
    "precision_macro_manual = 0\n",
    "precision_weighted_manual = 0\n",
    "total_soporte = 0\n",
    "\n",
    "for clase in clases:\n",
    "    TP = sum((yt == clase) and (yp == clase) for yt, yp in zip(y_test_multi, y_pred_multi))\n",
    "    FP = sum((yt != clase) and (yp == clase) for yt, yp in zip(y_test_multi, y_pred_multi))\n",
    "    FN = sum((yt == clase) and (yp != clase) for yt, yp in zip(y_test_multi, y_pred_multi))\n",
    "    soporte = sum((yt == clase) for yt in y_test_multi)  # Total de instancias reales de la clase\n",
    "    \n",
    "    precision_manual = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    precision_macro_manual += precision_manual\n",
    "    precision_weighted_manual += precision_manual * soporte\n",
    "    total_soporte += soporte\n",
    "    \n",
    "    resultados[clase] = {'TP': TP, 'FP': FP, 'FN': FN, 'Soporte': soporte, 'Precisión (manual)': precision_manual}\n",
    "\n",
    "# Calcular precisiones macro y ponderada manualmente\n",
    "precision_macro_manual /= len(clases)\n",
    "precision_weighted_manual /= total_soporte\n",
    "\n",
    "# Mostrar resultados manuales\n",
    "for clase, metricas in resultados.items():\n",
    "    print(f\"Clase: {clase}, Métricas: {metricas}\")\n",
    "\n",
    "print(f\"\\nPrecisión Macro (manual): {precision_macro_manual:.2f}\")\n",
    "print(f\"Precisión Ponderada (manual): {precision_weighted_manual:.2f}\")\n",
    "\n",
    "# Usando sklearn\n",
    "precision_macro = precision_score(y_test_multi, y_pred_multi, average='macro', labels=clases)\n",
    "precision_weighted = precision_score(y_test_multi, y_pred_multi, average='weighted', labels=clases)\n",
    "\n",
    "print(f\"\\nPrecisión Macro (sklearn): {precision_macro:.2f}\")\n",
    "print(f\"Precisión Ponderada (sklearn): {precision_weighted:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e8df5-a3a3-4526-8849-e456d76b77cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
